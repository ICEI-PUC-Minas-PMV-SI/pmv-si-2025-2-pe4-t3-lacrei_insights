# Documentação do Processo ETL

Este documento detalha o pipeline ETL do projeto, explicando cada etapa, função e operação dos principais arquivos Python.

## Sumário
- [Extração (Staging 1)](#extração-staging-1)
- [Transformação (Staging 2)](#transformação-staging-2)
- [Models](#models)
- [Marts](#marts)
- [Carga](#carga)

---

## Extração (Staging 1)

Arquivo: `app/routes/etl/staging1.py`

O Staging 1 é responsável por extrair dados do banco relacional de origem e popular o schema `lacreisaude_staging_01`.

### Funções Principais

#### `def _rodar_etl_staging1(conn)`
Executa todos os jobs de extração para staging 1.

```python
@bp_staging1.route('/upload/staging1', methods=['GET'])
def upload_staging1():
    ...
    result = _rodar_etl_staging1(conn)
    ...
```

#### Funções de Extração por Tabela
Cada tabela tem uma função dedicada, por exemplo:

```python
def _etl_lacreiid_appointment(conn):
    conn.execute(text("""
        CREATE TABLE IF NOT EXISTS lacreisaude_staging_01.lacreiid_appointment (...)
    """))
    ...
    # Extração dos dados do banco de origem
    ...
```

Essas funções:
- Garantem a existência da tabela destino
- Extraem dados do banco de origem
- Inserem no staging 1

---

## Transformação (Staging 2)

Arquivo: `app/routes/etl/staging2.py`

O Staging 2 transforma e normaliza os dados do Staging 1, aplicando regras de negócio e tipagem.


#### Funções de Transformação por Tabela
Exemplo:

```python
def _rodar_etl_appointment(conn):
    conn.execute(text("""
        CREATE TABLE IF NOT EXISTS lacreisaude_staging_02.lacreiid_appointment (...)
    """))
    ...
    upsert_sql = """
        WITH src_raw AS (...)
        ...
        INSERT INTO lacreisaude_staging_02.lacreiid_appointment ...
        ON CONFLICT (id) DO UPDATE ...
    """
    row = conn.execute(text(upsert_sql)).mappings().one()
    return {...}
```

Essas funções:
- Criam tabelas de destino no `staging_02`
- Transformam e limpam os dados (tipagem, normalização, deduplicação)
- Fazem upsert (insert/update) dos dados transformados

Outras funções seguem o mesmo padrão para cada tabela: `_rodar_etl_profile`, `_rodar_etl_user`, `_rodar_etl_clinic`, etc.

---

## Models

Arquivo: `app/routes/etl/model.py`

Responsável por consolidar e modelar os dados para uso analítico, criando tabelas de fatos e dimensões.

### Funções Principais

#### `def _rodar_etl_model(conn)`
Executa todos os jobs de modelagem.

```python
def _rodar_etl_model(conn):
    """
    Popula/atualiza o schema lacreisaude_model a partir da staging_02.
    - Usa chaves naturais com UNIQUE INDEX para garantir idempotência.
    - Todos os INSERTs usam ON CONFLICT (<colunas>) ... (não por nome de constraint).
    """

    # -------------------------------------------------------------------------
    # 0) Schema
    # -------------------------------------------------------------------------
    conn.execute(text("CREATE SCHEMA IF NOT EXISTS lacreisaude_model;"))

    # -------------------------------------------------------------------------
    # 1) DIM DATE
    # -------------------------------------------------------------------------
    conn.execute(text("""
        CREATE TABLE IF NOT EXISTS lacreisaude_model.dim_lacreisaude_date
        (
            date_id       INTEGER GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,
            calendar_date DATE NOT NULL UNIQUE,
            day           INTEGER NOT NULL,
            month         INTEGER NOT NULL,
            year          INTEGER NOT NULL,
            week          INTEGER NOT NULL,
            quarter       INTEGER NOT NULL
        );
    """))

    ...
```


## Marts

Arquivo: `app/routes/etl/mart.py`

Responsável por criar data marts otimizados para consumo por dashboards e BI.

### Funções Principais

#### `def _rodar_etl_mart(conn)`
Executa todos os jobs de criação de marts.

```python
def _rodar_etl_mart(conn):
    """
    Constrói/atualiza as tabelas de Data Mart a partir das tabelas da camada MODEL (schema: lacreisaude_model).
    - Tabelas: patients, patient_disability, professionals, professional_appointments
    - Idempotente: usa ON CONFLICT (PK) DO UPDATE
    - Assume que a MODEL (dim_*/fact_*) já foi populada no mesmo 'conn'
    """

    # ---------------------------------------------------------------------
    # 0) Schema da MART
    # ---------------------------------------------------------------------
    conn.execute(text(f"CREATE SCHEMA IF NOT EXISTS {MART_SCHEMA};"))

    # ---------------------------------------------------------------------
    # 1) MART: patients
    #     PK: (period_month, age_group, gender_identity, sexual_orientation, pronoun)
    # ---------------------------------------------------------------------
    conn.execute(text(f"""
        CREATE TABLE IF NOT EXISTS {MART_SCHEMA}.patients (
            period_month DATE NOT NULL,
            age_group VARCHAR(50),
            pronoun VARCHAR(50),
            gender_identity VARCHAR(100),
            sexual_orientation VARCHAR(100),
            total_patients INT,
            active_patients INT,
            inactive_patients INT,
            active_percentage NUMERIC(5,2),
            growth_rate NUMERIC(5,2),
            CONSTRAINT mart_patient_pk
                PRIMARY KEY (period_month, age_group, gender_identity, sexual_orientation, pronoun)
        );
    """))
    ...
```
Função principal que executa todos os jobs de criação de data marts, otimizados para consumo por dashboards e BI. A função cria as tabelas de mart e popula com dados agregados e prontos para análise, a partir das tabelas modeladas.

    - Cria cada tabela de destino
    - Realiza joins e agregações necessárias
    - Popula a tabela modelada

## Carga

A carga é realizada ao final de cada etapa, com as funções de upsert (insert/update) garantindo que os dados estejam sempre atualizados e sem duplicidade.

- O staging 1 carrega do banco relacional para staging_01
- O staging 2 transforma e carrega para staging_02
- O model.py carrega para tabelas de fatos e dimensões
- O mart.py carrega para data marts

---

## Observações
- O uso de `ON CONFLICT DO UPDATE` garante atualização incremental.
- O pipeline é modular: cada etapa pode ser executada isoladamente via endpoints Flask.

---


